---
title: "Data 607 Project 3"
date: "2022-10-14"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    number_sections: false
    highlight: pygments
    theme: cerulean
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)

```

## PART 1 -  Project 1 Description
Create a short document, with the names of group members. You should briefly describe your collaboration tool(s) you’ll use as a group, including for communication, code sharing, and project documentation. You should have identified your data sources, where the data can be found, and how to load it.  And you should have created at least a logical model for your normalized database, and produced an Entity-Relationship (ER) diagram documenting your database design


<br/>

### Project 3 Team Members

* Sanielle Worrell
* Vladimir Nimchenko
* Jose Rodriguez
* Johnny Rodriguez


> #### Team Insights
* Key to making progress was getting organized
* We met several times in Zoom working sessions and communicated over Slack
* Divyed-up parts of the project to members


<br/>

### Project Tools

The team is using R Studio Cloud (https://rstudio.cloud) for collaboration and code development.  This allows us to view and share code within the project. We are using R Markdown within RStudio Cloud for project documentation to publish through RPubs (https://rpubs.com) . To create the ERD, the team used Quick Database Diagrams (https://www.quickdatabasediagrams.com).  Source CSV and RMD files are saved in a github repo so files can be accessed centrally (https://github.com/johnnydrodriguez/data607_project3) In addition to this, the team communicates over Zoom and Slack.

<br/>

> #### Project Tools Insights
* Initially, we looked for collaboration tools that allowed for real time editing (like Google Docs, or Online MS Word).  Google Collab with the R engine was an option we experience some collaboration clunkiness with the tool
* Ultimately, RStudio Cloud was not a tool we could use for collaboration either for the project.  Despite setting up a collaborative work space and upgrading the service tier to increase performance and availability, the cloud version was too limiting for team use.
* The team began using Github as a central repository for shared code and collaborated in real time over Zoom sessions. This was effective in moving the project forward.


<br/>

### Project 3 Data Sources

The data used to answer the question is taken from Glassdoor via Kaggle.  The data was scrapped from Glassdoor and posted to the site.

Source:
https://www.kaggle.com/datasets/nikhilbhathi/data-scientist-salary-us-glassdoor?resource=download

Source CSV  (Uncleaned for Project): 
https://raw.githubusercontent.com/johnnydrodriguez/data607_project3/main/glassdoor_2021.csv

<br/>

> #### Data Sources Insights
* Finding datasets that answered this specific question we challening to find. 
* We correctly hypothesized that a possible way to locate date were from job sites (GlassDoor, Indeed, LinkedIn), and federal labor (Bureau of Labor Statistics) and higher education government sites (Integrated Postsecondary Education Data System) .  
* Job sites proved challenging as API or public data sets were not available -- at least not without significantly more effort performing scraping work or paid fees.  
* The Federal government sites are data-rich but job titles, skills and other variables that may have directly helped answer the question where typically aggregated generically under Data Science or Computer Information variables.
* The Glassdoor data used in the project was scraped and made available through Kaggle.


<br/>

### Project 3 Entity Relationship Diagram

https://github.com/johnnydrodriguez/data607_project3/blob/main/QuickDBD-export.png

<br/>

> #### ERD Insights
* The Quick Database diagram website was easy to use to create the ERD.  
* Once the project got underway, a discussion point was whether to adjust.  For example, our initial design required only 3 tables: Company, Job and Tech Skills.  However, the data included variable not available in their own columns - rather, they were buried as strings within the Job Descriptions.  
* This allowed us to identify two other types of skills - academic domain skills (Statistics, Math, Economics) and soft skills (problem-solving, critical thinking, decision-making, etc)
* We determined the these variable were best used as >>>> variable in their own tables >>>> additional columns in the original 3 table design.

<br/>



```{r img-with-knitr, echo=FALSE, fig.align='center', out.width='100%', fig.cap='Data Science Valuble Skills ERD'}

knitr::include_graphics("https://raw.githubusercontent.com/johnnydrodriguez/data607_project3/main/QuickDBD-export.png")

```


---

## PART 2 - Which are the most valued data science skills?

<br/>

### Prepping the Environment Project Packages and Libraries

```{r}


install.packages("tidyverse", repos = "http://cran.us.r-project.org")
install.packages("RMySQL", repos = "http://cran.us.r-project.org")
install.packages("ggplot2", repos = "http://cran.us.r-project.org")
install.packages("aws.s3", repos = "http://cran.us.r-project.org") 

library(tidyverse)
library(RMySQL)
library(ggplot2)
library(aws.s3)



```

<br/>

## Connecting to the SQL database and loading raw data
```{r}


#Connect to S3------------------------------------------------------------------

AWS_ACCESS_KEY_ID <- .rs.askForPassword("AWS Access Key ID:")
AWS_SECRET_ACCESS_KEY <- .rs.askForPassword("AWS Secret Access Key:")

#Use IAM login credentials/Set to "read-only"
Sys.setenv("AWS_ACCESS_KEY_ID" = "AKIA4QMABP5OYQY2NUG6",
           "AWS_SECRET_ACCESS_KEY" = "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")

#Check bucket
get_bucket("data-skills-p3")

#Load csv file from s3 into a df
jobs_df <- aws.s3::s3read_using(read.csv, object = "s3://data-skills-p3/glassdoor_2021.csv")


#Connect to RDS-----------------------------------------------------------------
#Input password:
password <- .rs.askForPassword("Database Password:")
endpoint <- "database-1.c4xyb2t3srpc.us-east-1.rds.amazonaws.com"
port <- 3306
username <- "admin"
dbname <- "database-1"
region <- "us-east-1a"

# Connect to the database using an IAM authentication token.
con <- DBI::dbConnect(
  RMySQL::MySQL(),
  host = "database-1.c4xyb2t3srpc.us-east-1.rds.amazonaws.com",
  port = 3306,
  db = "database1",
  user = "admin",
  password = password
)

## TEST DATABASE CONNECTION
class(con)

#List database tables
tables <- dbListTables(con) # load tables in rds to variable
str(tables) # Display structure of tables

#To create a table into the MySQL rds database
#dbWriteTable(conn = con, name = 'Test', value = as.data.frame(Thurstone))

#To drop a table from rds database
#dbRemoveTable(con,"iris") #Replace 2nd variable



knitr::kable(head(jobs_df))




```


<br/>

## Data clean up and transformation

```{r}
# Rename Columns
jobs_df <- jobs_df %>%
rename("job_title" = 2, "est_salary" = 3, "job_description" = 4, "company" = 6, "ownership" = 11)


# Remove duplicates
jobs_unique <- jobs_df %>% distinct(job_description,company, .keep_all = TRUE)

# Clean up commas in data
jobs_unique$job_description <- gsub("’", "'", jobs_unique$job_description)

# Identify keywords
education <- c("Masters", "Bachelors", "Master's", "Bachelor's", "Phd", "PhD", "Ph.D")
jobs_unique$education <- mapply(function(x)paste(education[str_detect(x,(education))],
                                        collapse = ","), jobs_unique$job_description)

# Skills Extraction from Job Description
jobs_unique <- jobs_unique %>%
  mutate(DA = str_match(job_description, '.*(Data Analy).*')[,2],
         R = str_match(job_description, '.*(\\sR\\s).*')[,2],
         API = str_match(job_description, '.*(API).*')[,2],
       #   Viz = str_match(job_description, '.*(Visualization).*')[,2],
       #   ML = str_match(job_description, '.*(Machine Learning | ML).*')[,2],
       #   LR = str_match(job_description, '.*(Logistic Regression).*')[,2],
       #   ETL = str_match(job_description, '.*(ETL).*')[,2],
        #  PD = str_match(job_description, '.*(Predictive).*')[,2],
          ST = str_match(job_description, '.*(Stat).*')[,2],
       #  EC = str_match(job_description, '.*(Economics).*')[,2],
        # ST = str_match(job_description, '.*(Math).*')[,2],
        # PS = str_match(job_description, '.*(problem-solving).*')[,2],
        # CM = str_match(job_description, '.*(communication).*')[,2],
       #  DM = str_match(job_description, '.*(decision-making).*')[,2],
        # CR = str_match(job_description, '.*(creativity).*')[,2],
         CT = str_match(job_description, '.*(critical thinking).*')[,2]
         )










knitr::kable(head(jobs_unique))

```





<br/>

## Analysis

---

<br/>

## Conclusion
